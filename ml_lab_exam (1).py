# -*- coding: utf-8 -*-
"""ML_LAB_exam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YAkoc-XqnNEXh0ENWbmhDJ3rYZtC77qu
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
!pip install nltk
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt_tab')

import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

import re
import string
import seaborn as sns
import matplotlib.pyplot as plt
import scipy.sparse

train=pd.read_csv('/content/training.1600000.processed.noemoticon.csv', encoding='latin1', header=None)
dftest=pd.read_csv('/content/testdata.manual.2009.06.14.csv', encoding='latin1', header=None)

# Assuming your DataFrame is named 'df'
train_columns = ['Polarity', 'id', 'date', 'query', 'user', 'tweet']
dftest_columns = ['Polarity', 'id', 'date', 'query', 'user', 'tweet']

train.head()

dftest.head()

train.shape

dftest.shape

dftest['Polarity'].value_counts()
# train['Polarity'].value_counts()

train['Polarity'].value_counts()

train1 = train[["Polarity", "tweet"]]
dftest1 = dftest[["Polarity", "tweet"]]

dftest1['Polarity'].value_counts()

train1['Polarity'].value_counts()

train1["Polarity"] = train1["Polarity"].replace(4,1)
train1["Polarity"] = train1["Polarity"].replace(0,-1)

dftest1["Polarity"]  = dftest1["Polarity"].replace(4,1)
dftest1["Polarity"]  = dftest1["Polarity"].replace(0,-1)
dftest1["Polarity"]  = dftest1["Polarity"].replace(2,0)

tweets_df = pd.concat([train1,dftest1], axis=0)

sns.countplot(data =tweets_df, x = "Polarity")

tweets_df["tweet"] = tweets_df["tweet"].astype(str)
tweets_df.reset_index(drop = True,inplace=True)

tweets_df["length"] = tweets_df["tweet"].apply(len)

tweets_df.groupby("Polarity")["length"].describe()

import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt_tab')

def clean_text(text):
    pat1 = r'@[^ ]+'
    pat2 = r'https?://[A-Za-z0-9./]+'
    pat3 = r'\'s'
    pat4 = r'\#\w+'
    pat5 = r'&amp '
    pat6 = r'[^A-Za-z\s]'
    combined_pat = r'|'.join((pat1, pat2,pat3,pat4,pat5, pat6))
    text = re.sub(combined_pat,"",text).lower()
    return text.strip()

tweets_df["cleaned_tweet"] = tweets_df["tweet"].apply(clean_text)

tweets_df = tweets_df [ ~(tweets_df["cleaned_tweet"] =="")]

lem = WordNetLemmatizer()

def tokenize_lem(sentence):
    outlist= []
    token = sentence.split()
    for tok in token:
        outlist.append(lem.lemmatize(tok))
    return " ".join(outlist)

tweets_df["cleaned_tweet"] = tweets_df["cleaned_tweet"].apply(tokenize_lem)

tweets_df.head()

train['date'] = pd.to_datetime(train['date'], errors='coerce')

sentiment_trends = train.groupby([pd.Grouper(key='date', freq='D'), 'Polarity']).size().unstack().fillna(0)

plt.figure(figsize=(12, 6))
for polarity in sentiment_trends.columns:
    plt.plot(sentiment_trends.index, sentiment_trends[polarity], label=f"Polarity {polarity}")

plt.title("Sentiment Trends Over Time")
plt.xlabel("Date")
plt.ylabel("Tweet Count")
plt.legend(title="Polarity")
plt.grid(True)
plt.show()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(tweets_df[["cleaned_tweet","length"]], tweets_df["Polarity"], test_size=0.1, random_state=42)

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer()
tfidf.fit(X_train["cleaned_tweet"])

X_train_v = tfidf.transform(X_train["cleaned_tweet"])
X_test_v = tfidf.transform(X_test["cleaned_tweet"])

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler2 = MinMaxScaler()

scaler.fit([X_train["length"]])
scaler2.fit([X_test["length"]])

X_train_len = scaler.transform([X_train["length"]])
X_train_len = X_train_len.reshape( X_train_v.shape[0], 1)

X_train = scipy.sparse.hstack([X_train_v,X_train_len], format = "csr")

joblib.dump(tfidf,"tidf_model.pkl")

joblib.dump(scaler2,"length_model.pkl")

X_train.shape

X_test.shape

# Instead of fitting on the entire X_test["length"], fit on a single sample:
scaler2.fit(X_test[["length"]][:1])

# Now, the transform should work:
X_test_len = scaler2.transform(X_test[["length"]])
X_test_len = X_test_len.reshape(X_test_v.shape[0], 1)

X_test = scipy.sparse.hstack([X_test_v,X_test_len], format = "csr")

import random

# Assuming X_train and y_train are your data and labels
# Get a list of indices to sample
sample_indices = random.sample(range(X_train.shape[0]), 500000)

# Reset index of y_train before sampling
y_train1 = y_train.reset_index(drop=True)

# Select the samples and labels using the indices
X_train_sampled = X_train[sample_indices]
y_train_sampled = y_train1[sample_indices] # Now this should work without KeyError

X_train_sampled.shape

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

lr=LogisticRegression()
lr.fit(X_train_sampled,y_train_sampled)
print(lr.score(X_test,y_test))

print(lr.score(X_train_sampled,y_train_sampled))

from sklearn.metrics import accuracy_score, recall_score, classification_report

y_pred=lr.predict(X_test)

acuracy=accuracy_score(y_test,y_pred)
acuracy

print(classification_report(y_test,y_pred))

import joblib

joblib.dump(lr,"Sentiment_model.pkl")

